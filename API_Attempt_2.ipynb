{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y numpy -q\n",
        "!pip install -q --no-cache-dir numpy==2.0.2\n",
        "\n",
        "!pip install -q --no-cache-dir \\\n",
        "  scipy==1.11.4 scikit-learn==1.4.2 pandas==2.2.2 \\\n",
        "  joblib==1.4.2 matplotlib==3.8.4 pyyaml==6.0.2 tqdm==4.67.1\n",
        "\n",
        "import os, sys\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "id": "KFe1EzFHCepp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy, scipy, sklearn, pandas as pd, joblib, matplotlib, yaml\n",
        "print(\"NumPy\", numpy.__version__+\",\",\n",
        "      \"SciPy\", scipy.__version__+\",\",\n",
        "      \"sklearn\", sklearn.__version__+\",\",\n",
        "      \"pandas\", pd.__version__+\",\",\n",
        "      \"matplotlib\", matplotlib.__version__+\",\",\n",
        "      \"PyYAML\", yaml.__version__)\n"
      ],
      "metadata": {
        "id": "wAmHPxIRC9N3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ],
      "metadata": {
        "id": "4zKy_mbJDcID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls \"/content/drive/MyDrive\" | head -n 20\n"
      ],
      "metadata": {
        "id": "prIdEggUDp4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "/content/drive/MyDrive/train.csv\n"
      ],
      "metadata": {
        "id": "xFwyL6weD-qS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "CSV_PATH = \"/content/drive/MyDrive/train.csv\"\n",
        "\n",
        "assert os.path.exists(CSV_PATH), f\"train.csv not found at: {CSV_PATH}\"\n",
        "\n",
        "df_raw = pd.read_csv(CSV_PATH)\n",
        "print(\"Dataset loaded:\", df_raw.shape)\n",
        "df_raw.head(10)\n"
      ],
      "metadata": {
        "id": "7LeYC8tnD_x7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Columns:\", df_raw.columns.tolist())\n",
        "print(\"\\n Null values per column:\")\n",
        "print(df_raw.isna().sum())\n"
      ],
      "metadata": {
        "id": "BS9_PSOuEaPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = df_raw.dropna(subset=[\"comment_text\"]).reset_index(drop=True)\n",
        "print(\"Cleaned dataset:\", df_raw.shape)\n"
      ],
      "metadata": {
        "id": "s7qzFMFMEc9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def normalize_text(text: str) -> str:\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+\", \" \", text)           # remove URLs\n",
        "    text = re.sub(r\"[^a-z\\s]\", \" \", text)          # keep only letters/spaces\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()       # collapse extra spaces\n",
        "    return text\n",
        "\n",
        "df_raw[\"text_clean\"] = df_raw[\"comment_text\"].astype(str).map(normalize_text)\n",
        "\n",
        "df_raw[[\"comment_text\", \"text_clean\"]].head(10)\n"
      ],
      "metadata": {
        "id": "CyCBHaFhEpyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_cols = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
        "\n",
        "# sanity check\n",
        "assert all(col in df_raw.columns for col in label_cols), \"Missing expected label columns.\"\n",
        "\n",
        "# 1 if any toxic label is 1, else 0\n",
        "df_raw[\"foul\"] = (df_raw[label_cols].sum(axis=1) > 0).astype(int)\n",
        "\n",
        "print(\"Binary foul label Ready\")\n",
        "df_raw[\"foul\"].value_counts(normalize=True)\n"
      ],
      "metadata": {
        "id": "Nxh5PvgME4v3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Features & target\n",
        "X = df_raw[\"text_clean\"]\n",
        "y = df_raw[\"foul\"]\n",
        "\n",
        "# 70 / 15 / 15 split\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test   = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp)\n",
        "\n",
        "print(f\"Split complete: train={len(X_train)}, val={len(X_val)}, test={len(X_test)}\")\n"
      ],
      "metadata": {
        "id": "KDGA9SGXFEzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(\n",
        "    max_features=30000,       # cap feature size\n",
        "    ngram_range=(1,2),        # unigrams + bigrams\n",
        "    min_df=3,                 # ignore rare terms\n",
        "    max_df=0.90,              # ignore extremely frequent words\n",
        "    sublinear_tf=True,        # better scaling for long texts\n",
        "    stop_words=\"english\"\n",
        ")\n",
        "\n",
        "X_train_vec = tfidf.fit_transform(X_train)\n",
        "X_val_vec   = tfidf.transform(X_val)\n",
        "X_test_vec  = tfidf.transform(X_test)\n",
        "\n",
        "print(\"TF-IDF shapes:\")\n",
        "print(\"Train:\", X_train_vec.shape, \"Val:\", X_val_vec.shape, \"Test:\", X_test_vec.shape)\n"
      ],
      "metadata": {
        "id": "wC7rph59FjXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier, HistGradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "0M-GoixoG6gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(name, model, X_val, y_val, y_pred, y_proba=None):\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(y_val, y_pred, average=\"macro\", zero_division=0)\n",
        "    prec_w, rec_w, f1_w, _ = precision_recall_fscore_support(y_val, y_pred, average=\"weighted\", zero_division=0)\n",
        "    auc = roc_auc_score(y_val, y_proba) if y_proba is not None else np.nan\n",
        "\n",
        "    print(f\"ðŸ”¹ {name}\")\n",
        "    print(f\"   Accuracy: {acc:.4f}\")\n",
        "    print(f\"   Macro Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
        "    print(f\"   Weighted Precision: {prec_w:.4f}, Recall: {rec_w:.4f}, F1: {f1_w:.4f}\")\n",
        "    print(f\"   ROC-AUC: {auc:.4f}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    return {\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": acc,\n",
        "        \"Precision_macro\": prec,\n",
        "        \"Recall_macro\": rec,\n",
        "        \"F1_macro\": f1,\n",
        "        \"Precision_weighted\": prec_w,\n",
        "        \"Recall_weighted\": rec_w,\n",
        "        \"F1_weighted\": f1_w,\n",
        "        \"ROC-AUC\": auc\n",
        "    }\n"
      ],
      "metadata": {
        "id": "Z8X-XmeKG-B-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "# 1. Logistic Regression\n",
        "logreg = LogisticRegression(max_iter=300)\n",
        "logreg.fit(X_train_vec, y_train)\n",
        "y_pred = logreg.predict(X_val_vec)\n",
        "y_proba = logreg.predict_proba(X_val_vec)[:,1]\n",
        "results.append(evaluate_model(\"Logistic Regression\", logreg, X_val, y_val, y_pred, y_proba))\n",
        "\n",
        "# 2. Linear SVM (note: no probability output)\n",
        "svm = LinearSVC()\n",
        "svm.fit(X_train_vec, y_train)\n",
        "y_pred = svm.predict(X_val_vec)\n",
        "results.append(evaluate_model(\"Linear SVM\", svm, X_val, y_val, y_pred))\n",
        "\n",
        "# 3. Multinomial NaÃ¯ve Bayes\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_vec, y_train)\n",
        "y_pred = nb.predict(X_val_vec)\n",
        "y_proba = nb.predict_proba(X_val_vec)[:,1]\n",
        "results.append(evaluate_model(\"Multinomial NB\", nb, X_val, y_val, y_pred, y_proba))\n",
        "\n",
        "# 4. Random Forest\n",
        "rf = RandomForestClassifier(n_estimators=100, n_jobs=-1, random_state=42)\n",
        "rf.fit(X_train_vec, y_train)\n",
        "y_pred = rf.predict(X_val_vec)\n",
        "y_proba = rf.predict_proba(X_val_vec)[:,1]\n",
        "results.append(evaluate_model(\"Random Forest\", rf, X_val, y_val, y_pred, y_proba))\n",
        "\n",
        "# 5. Gradient Boosting (fast & memory-efficient)\n",
        "gb = HistGradientBoostingClassifier(max_iter=100, random_state=42)\n",
        "gb.fit(X_train_vec.toarray(), y_train)  # requires dense input\n",
        "y_pred = gb.predict(X_val_vec.toarray())\n",
        "y_proba = gb.predict_proba(X_val_vec.toarray())[:,1]\n",
        "results.append(evaluate_model(\"Gradient Boosting\", gb, X_val, y_val, y_pred, y_proba))\n",
        "\n",
        "# show summary table\n",
        "pd.DataFrame(results).set_index(\"Model\").sort_values(\"F1_macro\", ascending=False)\n"
      ],
      "metadata": {
        "id": "2yJl8FM0HCV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RAM CRASH"
      ],
      "metadata": {
        "id": "pMHV6XIqLmhr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "CSV_PATH = \"/content/drive/MyDrive/train.csv\"\n",
        "df_raw = pd.read_csv(CSV_PATH)\n",
        "label_cols = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
        "df_raw[\"text_clean\"] = df_raw[\"comment_text\"].astype(str).str.lower()\n",
        "df_raw[\"foul\"] = (df_raw[label_cols].sum(axis=1) > 0).astype(int)\n",
        "\n",
        "X = df_raw[\"text_clean\"]\n",
        "y = df_raw[\"foul\"]\n",
        "\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.30, random_state=42, stratify=y)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.50, random_state=42, stratify=y_temp)\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(max_features=30000, ngram_range=(1,2), min_df=3, max_df=0.9, sublinear_tf=True, stop_words=\"english\")\n",
        "X_train_vec = tfidf.fit_transform(X_train)\n",
        "X_val_vec = tfidf.transform(X_val)\n",
        "\n",
        "print(\"TF-IDF matrices:\", X_train_vec.shape, X_val_vec.shape)\n"
      ],
      "metadata": {
        "id": "qrCuO8jJJ2Ts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosting on reduced features via TruncatedSVD\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "\n",
        "# Trying to reuse evaluate_model if it exists; otherwise define a minimal one\n",
        "try:\n",
        "    evaluate_model\n",
        "except NameError:\n",
        "    def evaluate_model(name, model, X_val, y_val, y_pred, y_proba=None):\n",
        "        acc = accuracy_score(y_val, y_pred)\n",
        "        prec, rec, f1, _ = precision_recall_fscore_support(y_val, y_pred, average=\"macro\", zero_division=0)\n",
        "        prec_w, rec_w, f1_w, _ = precision_recall_fscore_support(y_val, y_pred, average=\"weighted\", zero_division=0)\n",
        "        auc = roc_auc_score(y_val, y_proba) if y_proba is not None else np.nan\n",
        "        print(f\"ðŸ”¹ {name}\")\n",
        "        print(f\"   Accuracy: {acc:.4f}\")\n",
        "        print(f\"   Macro Precision: {prec:.4f}, Recall: {rec:.4f}, F1: {f1:.4f}\")\n",
        "        print(f\"   Weighted Precision: {prec_w:.4f}, Recall: {rec_w:.4f}, F1: {f1_w:.4f}\")\n",
        "        print(f\"   ROC-AUC: {auc:.4f}\")\n",
        "        print(\"-\"*60)\n",
        "        return {\n",
        "            \"Model\": name,\n",
        "            \"Accuracy\": acc,\n",
        "            \"Precision_macro\": prec,\n",
        "            \"Recall_macro\": rec,\n",
        "            \"F1_macro\": f1,\n",
        "            \"Precision_weighted\": prec_w,\n",
        "            \"Recall_weighted\": rec_w,\n",
        "            \"F1_weighted\": f1_w,\n",
        "            \"ROC-AUC\": auc\n",
        "        }\n",
        "\n",
        "\n",
        "try:\n",
        "    results\n",
        "except NameError:\n",
        "    results = []\n",
        "\n",
        "\n",
        "n_components = 512\n",
        "svd = TruncatedSVD(n_components=n_components, random_state=42)\n",
        "X_train_red = svd.fit_transform(X_train_vec)\n",
        "X_val_red   = svd.transform(X_val_vec)\n",
        "\n",
        "gb = HistGradientBoostingClassifier(max_iter=150, random_state=42)\n",
        "gb.fit(X_train_red, y_train)\n",
        "y_pred = gb.predict(X_val_red)\n",
        "# use [:,1] for positive class prob\n",
        "y_proba = gb.predict_proba(X_val_red)[:, 1]\n",
        "\n",
        "results.append(evaluate_model(f\"Gradient Boosting (SVD {n_components})\", gb, X_val, y_val, y_pred, y_proba))\n",
        "\n",
        "# Summary table\n",
        "summary = pd.DataFrame(results).set_index(\"Model\").sort_values(\"F1_macro\", ascending=False)\n",
        "print(\"\\n Validation Summary (all models):\")\n",
        "display(summary)\n"
      ],
      "metadata": {
        "id": "kVYNTj12IsoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
        "import numpy as np, pandas as pd\n",
        "\n",
        "def evaluate_model(name, model, X_val, y_val, y_pred, y_proba=None):\n",
        "    acc = accuracy_score(y_val, y_pred)\n",
        "    p_m, r_m, f1_m, _ = precision_recall_fscore_support(y_val, y_pred, average=\"macro\", zero_division=0)\n",
        "    p_w, r_w, f1_w, _ = precision_recall_fscore_support(y_val, y_pred, average=\"weighted\", zero_division=0)\n",
        "    auc = roc_auc_score(y_val, y_proba) if y_proba is not None else np.nan\n",
        "    print(f\" {name}: acc={acc:.4f}, F1_macro={f1_m:.4f}, F1_weighted={f1_w:.4f}, AUC={auc:.4f}\")\n",
        "    return {\"Model\": name, \"Accuracy\": acc, \"F1_macro\": f1_m, \"F1_weighted\": f1_w, \"ROC-AUC\": auc}\n",
        "\n",
        "results = []\n",
        "\n",
        "# 1. Logistic Regression\n",
        "logreg = LogisticRegression(max_iter=300, solver=\"saga\", n_jobs=-1, random_state=42)\n",
        "logreg.fit(X_train_vec, y_train)\n",
        "results.append(evaluate_model(\"Logistic Regression\",\n",
        "                              logreg, X_val, y_val,\n",
        "                              logreg.predict(X_val_vec),\n",
        "                              logreg.predict_proba(X_val_vec)[:,1]))\n",
        "\n",
        "# 2. Linear SVM\n",
        "svm = LinearSVC(random_state=42)\n",
        "svm.fit(X_train_vec, y_train)\n",
        "results.append(evaluate_model(\"Linear SVM\", svm, X_val, y_val, svm.predict(X_val_vec)))\n",
        "\n",
        "# 3. Multinomial NB\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_vec, y_train)\n",
        "results.append(evaluate_model(\"Multinomial NB\",\n",
        "                              nb, X_val, y_val,\n",
        "                              nb.predict(X_val_vec),\n",
        "                              nb.predict_proba(X_val_vec)[:,1]))\n",
        "\n",
        "# 4. Random Forest (reduced size)\n",
        "rf = RandomForestClassifier(n_estimators=100, max_depth=12, n_jobs=-1, random_state=42)\n",
        "rf.fit(X_train_red, y_train)\n",
        "results.append(evaluate_model(\"Random Forest (SVD 512)\",\n",
        "                              rf, X_val, y_val,\n",
        "                              rf.predict(X_val_red),\n",
        "                              rf.predict_proba(X_val_red)[:,1]))\n",
        "\n",
        "# 5. Gradient Boosting (SVD 512)\n",
        "try:\n",
        "    gb, X_val_red\n",
        "    y_pred = gb.predict(X_val_red)\n",
        "    y_proba = gb.predict_proba(X_val_red)[:,1]\n",
        "    results.append(evaluate_model(\"Gradient Boosting (SVD 512)\", gb, X_val, y_val, y_pred, y_proba))\n",
        "except Exception as e:\n",
        "\n",
        "\n",
        "summary = pd.DataFrame(results).set_index(\"Model\").sort_values(\"F1_macro\", ascending=False)\n",
        "print(\"\\n Validation Summary:\")\n",
        "display(summary)\n"
      ],
      "metadata": {
        "id": "TGexzDzdL_eT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd, joblib\n",
        "from sklearn.metrics import (\n",
        "    classification_report, confusion_matrix, roc_auc_score, average_precision_score,\n",
        "    precision_recall_fscore_support, roc_curve, precision_recall_curve\n",
        ")\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "\n",
        "logreg = LogisticRegression(max_iter=1000, solver=\"saga\", n_jobs=-1, random_state=42)\n",
        "logreg.fit(X_train_vec, y_train)\n",
        "p_val = logreg.predict_proba(X_val_vec)[:, 1]\n",
        "\n",
        "\n",
        "def choose_threshold(probs, y_true, recall_target=0.80):\n",
        "    best = {\"thr\":0.5, \"precision\":0.0, \"recall\":0.0, \"f1\":0.0}\n",
        "    for thr in np.linspace(0.05, 0.95, 19):\n",
        "        y_hat = (probs >= thr).astype(int)\n",
        "        p, r, f1, _ = precision_recall_fscore_support(y_true, y_hat, average=\"binary\", zero_division=0)\n",
        "\n",
        "        if r >= recall_target and (p > best[\"precision\"] or (p==best[\"precision\"] and f1 > best[\"f1\"])):\n",
        "            best = {\"thr\": float(thr), \"precision\": float(p), \"recall\": float(r), \"f1\": float(f1)}\n",
        "\n",
        "    if best[\"precision\"] == 0.0 and best[\"recall\"] == 0.0:\n",
        "        y_hat = (probs >= 0.5).astype(int)\n",
        "        p, r, f1, _ = precision_recall_fscore_support(y_true, y_hat, average=\"binary\", zero_division=0)\n",
        "        best = {\"thr\": 0.5, \"precision\": float(p), \"recall\": float(r), \"f1\": float(f1)}\n",
        "    return best\n",
        "\n",
        "thr_info = choose_threshold(p_val, y_val, recall_target=0.80)\n",
        "THR = thr_info[\"thr\"]\n",
        "print(f\"Chosen threshold on validation: {THR:.2f}  (precision={thr_info['precision']:.3f}, recall={thr_info['recall']:.3f}, f1={thr_info['f1']:.3f})\")\n",
        "\n",
        "# 3) Refit TF-IDF + LogReg on TRAIN+VAL, Then TEST\n",
        "X_trval = pd.concat([X_train, X_val], axis=0)\n",
        "y_trval = pd.concat([y_train, y_val], axis=0)\n",
        "\n",
        "tfidf_final = TfidfVectorizer(\n",
        "    max_features=30000, ngram_range=(1,2), min_df=3, max_df=0.90,\n",
        "    sublinear_tf=True, stop_words=\"english\"\n",
        ")\n",
        "X_trval_vec = tfidf_final.fit_transform(X_trval)\n",
        "X_test_vec  = tfidf_final.transform(X_test)\n",
        "\n",
        "logreg_final = LogisticRegression(max_iter=1000, solver=\"saga\", n_jobs=-1, random_state=42)\n",
        "logreg_final.fit(X_trval_vec, y_trval)\n",
        "\n",
        "p_test = logreg_final.predict_proba(X_test_vec)[:, 1]\n",
        "y_hat  = (p_test >= THR).astype(int)\n",
        "\n",
        "# 4) Full test metrics\n",
        "report = classification_report(y_test, y_hat, target_names=[\"proper(0)\",\"foul(1)\"], zero_division=0)\n",
        "cm = confusion_matrix(y_test, y_hat)\n",
        "roc = roc_auc_score(y_test, p_test)\n",
        "pr_auc = average_precision_score(y_test, p_test)\n",
        "\n",
        "p_m, r_m, f1_m, _ = precision_recall_fscore_support(y_test, y_hat, average=\"macro\", zero_division=0)\n",
        "p_w, r_w, f1_w, _ = precision_recall_fscore_support(y_test, y_hat, average=\"weighted\", zero_division=0)\n",
        "acc = (y_test == y_hat).mean()\n",
        "\n",
        "print(\"\\n=== TEST METRICS (Logistic Regression @ tuned threshold) ===\")\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Macro  P/R/F1: {p_m:.4f} / {r_m:.4f} / {f1_m:.4f}\")\n",
        "print(f\"Weight P/R/F1: {p_w:.4f} / {r_w:.4f} / {f1_w:.4f}\")\n",
        "print(f\"ROC-AUC: {roc:.4f}   PR-AUC: {pr_auc:.4f}\")\n",
        "print(\"Confusion matrix:\\n\", cm)\n",
        "print(\"\\nDetailed classification report:\\n\", report)\n",
        "\n",
        "# Pipeline:\n",
        "pipe = Pipeline([\n",
        "    (\"tfidf\", tfidf_final),\n",
        "    (\"clf\", logreg_final)\n",
        "])\n",
        "\n",
        "import os\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "WIN_PATH = \"models/best_model_logreg_pipeline.pkl\"\n",
        "joblib.dump({\"pipeline\": pipe, \"threshold\": THR}, WIN_PATH)\n",
        "print(\"\\n Saved pipeline to:\", WIN_PATH)\n"
      ],
      "metadata": {
        "id": "sCMBPC_oUlQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, pandas as pd, joblib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.metrics import classification_report, average_precision_score, roc_auc_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "label_cols = [\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]\n",
        "\n",
        "Y = df_raw[label_cols].astype(int)\n",
        "\n",
        "Y_train = Y.loc[X_train.index]\n",
        "Y_val   = Y.loc[X_val.index]\n",
        "Y_test  = Y.loc[X_test.index]\n",
        "\n",
        "tfidf_multi = TfidfVectorizer(\n",
        "    max_features=30000, ngram_range=(1,2), min_df=3, max_df=0.90,\n",
        "    sublinear_tf=True, stop_words=\"english\"\n",
        ")\n",
        "\n",
        "X_train_vec_m = tfidf_multi.fit_transform(X_train)\n",
        "X_val_vec_m   = tfidf_multi.transform(X_val)\n",
        "X_test_vec_m  = tfidf_multi.transform(X_test)\n",
        "\n",
        "base_lr = LogisticRegression(max_iter=1000, solver=\"saga\", n_jobs=-1, random_state=42)\n",
        "multi_clf = OneVsRestClassifier(base_lr)\n",
        "multi_clf.fit(X_train_vec_m, Y_train)\n",
        "\n",
        "def choose_threshold_per_label(probs_col, y_true_col, recall_target=0.80):\n",
        "    best = {\"thr\":0.5, \"precision\":0.0, \"recall\":0.0, \"f1\":0.0}\n",
        "\n",
        "    for thr in np.linspace(0.05, 0.95, 19):\n",
        "        y_hat = (probs_col >= thr).astype(int)\n",
        "        tp = ((y_hat==1) & (y_true_col==1)).sum()\n",
        "        fp = ((y_hat==1) & (y_true_col==0)).sum()\n",
        "        fn = ((y_hat==0) & (y_true_col==1)).sum()\n",
        "        precision = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
        "        recall    = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
        "        f1        = (2*precision*recall)/(precision+recall) if (precision+recall)>0 else 0.0\n",
        "        if recall >= recall_target and (precision > best[\"precision\"] or (precision==best[\"precision\"] and f1>best[\"f1\"])):\n",
        "            best = {\"thr\": float(thr), \"precision\": float(precision), \"recall\": float(recall), \"f1\": float(f1)}\n",
        "    if best[\"precision\"]==0.0 and best[\"recall\"]==0.0:\n",
        "\n",
        "        thr = 0.5\n",
        "        y_hat = (probs_col >= thr).astype(int)\n",
        "        tp = ((y_hat==1) & (y_true_col==1)).sum()\n",
        "        fp = ((y_hat==1) & (y_true_col==0)).sum()\n",
        "        fn = ((y_hat==0) & (y_true_col==1)).sum()\n",
        "        precision = tp / (tp + fp) if (tp+fp)>0 else 0.0\n",
        "        recall    = tp / (tp + fn) if (tp+fn)>0 else 0.0\n",
        "        f1        = (2*precision*recall)/(precision+recall) if (precision+recall)>0 else 0.0\n",
        "        best = {\"thr\": thr, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
        "    return best\n",
        "\n",
        "\n",
        "P_val = multi_clf.predict_proba(X_val_vec_m)\n",
        "per_label_thr = {}\n",
        "per_label_stats = []\n",
        "for i, lab in enumerate(label_cols):\n",
        "    info = choose_threshold_per_label(P_val[:, i], Y_val[lab].values, recall_target=0.80)\n",
        "    per_label_thr[lab] = float(info[\"thr\"])\n",
        "    per_label_stats.append({\"label\": lab, **info})\n",
        "pd.DataFrame(per_label_stats)\n",
        "print(\"Chosen per-label thresholds:\\n\", per_label_thr)\n",
        "\n",
        "\n",
        "P_test = multi_clf.predict_proba(X_test_vec_m)\n",
        "Y_hat_test = np.zeros_like(P_test, dtype=int)\n",
        "for i, lab in enumerate(label_cols):\n",
        "    Y_hat_test[:, i] = (P_test[:, i] >= per_label_thr[lab]).astype(int)\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score\n",
        "macro_f1 = f1_score(Y_test, Y_hat_test, average=\"macro\", zero_division=0)\n",
        "weighted_f1 = f1_score(Y_test, Y_hat_test, average=\"weighted\", zero_division=0)\n",
        "macro_prec = precision_score(Y_test, Y_hat_test, average=\"macro\", zero_division=0)\n",
        "macro_rec  = recall_score(Y_test, Y_hat_test, average=\"macro\", zero_division=0)\n",
        "\n",
        "\n",
        "roc_aucs = []\n",
        "pr_aucs  = []\n",
        "for i, lab in enumerate(label_cols):\n",
        "    try:\n",
        "        roc_aucs.append(roc_auc_score(Y_test[lab], P_test[:, i]))\n",
        "        pr_aucs.append(average_precision_score(Y_test[lab], P_test[:, i]))\n",
        "    except ValueError:\n",
        "\n",
        "        pass\n",
        "\n",
        "print(\"\\n=== MULTI-LABEL TEST METRICS ===\")\n",
        "print(f\"Macro Precision: {macro_prec:.4f}  Macro Recall: {macro_rec:.4f}  Macro F1: {macro_f1:.4f}  Weighted F1: {weighted_f1:.4f}\")\n",
        "if roc_aucs:\n",
        "    print(f\"ROC-AUC (macro over labels): {np.mean(roc_aucs):.4f}\")\n",
        "if pr_aucs:\n",
        "    print(f\"PR-AUC  (macro over labels): {np.mean(pr_aucs):.4f}\")\n",
        "\n",
        "\n",
        "pipe_multi = Pipeline([\n",
        "    (\"tfidf\", tfidf_multi),\n",
        "    (\"clf\", OneVsRestClassifier(LogisticRegression(max_iter=1000, solver='saga', n_jobs=-1, random_state=42)))\n",
        "])\n",
        "pipe_multi.fit(pd.concat([X_train, X_val]), pd.concat([Y_train, Y_val]))\n",
        "\n",
        "import os\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "ML_PATH = \"models/multi_model_pipeline.pkl\"\n",
        "joblib.dump({\n",
        "    \"pipeline\": pipe_multi,       # TF-IDF + OneVsRest(LogReg)\n",
        "    \"labels\": label_cols,         # label order\n",
        "    \"thresholds\": per_label_thr   # per-label tuned thresholds (floats)\n",
        "}, ML_PATH)\n",
        "print(\"\\n Saved multi-label pipeline to:\", ML_PATH)\n"
      ],
      "metadata": {
        "id": "rYjO-IdAWOhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, joblib, numpy as np, pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "BIN_ART = joblib.load(\"models/best_model_logreg_pipeline.pkl\")\n",
        "BIN_PIPE = BIN_ART[\"pipeline\"]\n",
        "BIN_THR  = float(BIN_ART[\"threshold\"])\n",
        "\n",
        "ML_ART  = joblib.load(\"models/multi_model_pipeline.pkl\")\n",
        "ML_PIPE = ML_ART[\"pipeline\"]            # TF-IDF + OneVsRest(LogReg)\n",
        "LABELS  = list(ML_ART[\"labels\"])\n",
        "THR     = {k: float(v) for k, v in ML_ART[\"thresholds\"].items()}\n",
        "\n",
        "def normalize(text: str) -> str:\n",
        "    t = text.lower()\n",
        "    t = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", t)\n",
        "    t = re.sub(r\"[^a-z\\s]\", \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t\n",
        "\n",
        "LEX_BAD = {\n",
        "    # obscene\n",
        "    \"fuck\": 1.5, \"fucking\": 1.5, \"shit\": 1.0, \"bitch\": 1.3, \"slut\": 1.5,\n",
        "    # harassment / insults\n",
        "    \"idiot\": 0.8, \"stupid\": 0.7, \"moron\": 0.7, \"dumb\": 0.6,\n",
        "    # violence\n",
        "    \"kill\": 2.0, \"die\": 1.5, \"murder\": 2.2, \"hang\": 2.0,\n",
        "    # sexual harassment terms (non-obscene)\n",
        "    \"creep\": 0.7, \"pervert\": 1.0,\n",
        "}\n",
        "\n",
        "# Identity/race group keywords\n",
        "IDENTITY_GROUPS = {\n",
        "    \"black\": \"Racist language\",\n",
        "    \"blacks\": \"Racist language\",\n",
        "    \"asian\": \"Racist language\",\n",
        "    \"asians\": \"Racist language\",\n",
        "    \"jew\": \"Antisemitic language\",\n",
        "    \"jews\": \"Antisemitic language\",\n",
        "    \"muslim\": \"Islamophobic language\",\n",
        "    \"muslims\": \"Islamophobic language\",\n",
        "    \"gay\": \"Homophobic language\",\n",
        "    \"gays\": \"Homophobic language\",\n",
        "    \"lesbian\": \"Homophobic language\",\n",
        "    \"lesbians\": \"Homophobic language\",\n",
        "}\n",
        "\n",
        "# Base tags directly tied to labels\n",
        "BASE_TAGS = {\n",
        "    \"toxic\": \"Abusive / toxic language\",\n",
        "    \"severe_toxic\": \"Severe abuse\",\n",
        "    \"obscene\": \"Obscene language\",\n",
        "    \"threat\": \"Violence / threats\",\n",
        "    \"insult\": \"Harassment / insult\",\n",
        "    \"identity_hate\": \"Hate speech / identity-based\"\n",
        "}\n",
        "\n",
        "def detect_identity_subtags(text_tokens):\n",
        "    \"\"\"Return additional identity-specific tags like 'Racist language' when identity terms appear.\"\"\"\n",
        "    tags = set()\n",
        "    for tok in text_tokens:\n",
        "        if tok in IDENTITY_GROUPS:\n",
        "            tags.add(IDENTITY_GROUPS[tok])\n",
        "    return sorted(tags)\n",
        "\n",
        "# Foulness meter (0 - 10)\n",
        "def foulness_meter_0_10(text_norm: str, probs: np.ndarray, labels: list[str], thresholds: dict[str, float]) -> int:\n",
        "    \"\"\"\n",
        "    Intuition:\n",
        "      - Start with max(prob) scaled to 0..6\n",
        "      - Add lexicon intensity (sum of weights for matched bad words, capped) up to ~2.5\n",
        "      - Add repetition boost if the same bad word repeats (up to ~1.0)\n",
        "      - Add density bonus: how many labels exceed their thresholds (up to ~1.0)\n",
        "      - Clip to [0,10]\n",
        "    \"\"\"\n",
        "\n",
        "    base = 6.0 * float(np.max(probs))\n",
        "\n",
        "\n",
        "    toks = re.findall(r\"\\b[a-z]+\\b\", text_norm)\n",
        "    weights = [LEX_BAD[w] for w in toks if w in LEX_BAD]\n",
        "    lex_intensity = min(sum(weights), 2.5)\n",
        "\n",
        "\n",
        "    cnt = Counter([w for w in toks if w in LEX_BAD])\n",
        "    rep = max(cnt.values()) if cnt else 1\n",
        "    rep_boost = min(0.5 * (rep - 1), 1.0)\n",
        "\n",
        "\n",
        "    num_on = sum(float(probs[i]) >= thresholds[labels[i]] for i in range(len(labels)))\n",
        "    density = min(0.4 * num_on, 1.0)\n",
        "\n",
        "    score = base + lex_intensity + rep_boost + density\n",
        "    return int(round(max(0.0, min(10.0, score))))\n",
        "\n",
        "# tag generation\n",
        "def analyze_tweet(text: str):\n",
        "    \"\"\"\n",
        "    Returns a dict:\n",
        "      - binary: prob + tuned label (winner pipeline)\n",
        "      - subtypes: per-label prob + label (using per-label thresholds)\n",
        "      - tags: list of human-friendly tags\n",
        "      - foulness_meter: 0..10 score\n",
        "    \"\"\"\n",
        "    norm = normalize(text)\n",
        "\n",
        "\n",
        "    p_bin = float(BIN_PIPE.predict_proba([norm])[0, 1])\n",
        "    bin_label = int(p_bin >= BIN_THR)\n",
        "\n",
        "\n",
        "    P = ML_PIPE.predict_proba([norm])[0]\n",
        "    subtypes = {\n",
        "        lab: {\"prob\": float(P[i]), \"label\": int(float(P[i]) >= THR[lab])}\n",
        "        for i, lab in enumerate(LABELS)\n",
        "    }\n",
        "\n",
        "\n",
        "    active = [BASE_TAGS[lab] for lab in LABELS if subtypes[lab][\"label\"] == 1]\n",
        "\n",
        "    toks = re.findall(r\"\\b[a-z]+\\b\", norm)\n",
        "    if subtypes.get(\"identity_hate\", {}).get(\"label\", 0) == 1 or any(tok in IDENTITY_GROUPS for tok in toks):\n",
        "        active.extend(detect_identity_subtags(toks))\n",
        "\n",
        "        if \"Hate speech / identity-based\" not in active:\n",
        "            active.append(\"Hate speech / identity-based\")\n",
        "\n",
        "\n",
        "    seen = set(); tags = []\n",
        "    for t in active:\n",
        "        if t not in seen:\n",
        "            seen.add(t); tags.append(t)\n",
        "\n",
        "\n",
        "    meter = foulness_meter_0_10(norm, np.array(P), LABELS, THR)\n",
        "\n",
        "    return {\n",
        "        \"binary\": {\"prob\": p_bin, \"threshold\": BIN_THR, \"label\": bin_label},\n",
        "        \"subtypes\": subtypes,\n",
        "        \"tags\": tags,\n",
        "        \"foulness_meter\": meter\n",
        "    }\n",
        "\n",
        "# test\n",
        "examples = [\n",
        "    \"Have a wonderful day everyone!\",\n",
        "    \"you idiot, fuck off\",\n",
        "    \"I hope you die tonight\",\n",
        "    \"I hate Black people, they should kill each other\",\n",
        "    \"stop being stupid stupid stupid\",\n",
        "    \"what a creep, pervert vibes\",\n",
        "]\n",
        "\n",
        "for s in examples:\n",
        "    out = analyze_tweet(s)\n",
        "    print(\"\\nTEXT:\", s)\n",
        "    print(\" binary:\", out[\"binary\"])\n",
        "    print(\" meter:\", out[\"foulness_meter\"])\n",
        "    print(\" tags :\", out[\"tags\"])\n",
        "\n",
        "    fired = {k:v for k,v in out[\"subtypes\"].items() if v[\"label\"]==1}\n",
        "    print(\" subtypes_on:\", fired if fired else \"{}\")\n"
      ],
      "metadata": {
        "id": "u5_XNC_0Xz6A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"models/best_model_logreg_pipeline.pkl\")\n",
        "files.download(\"models/multi_model_pipeline.pkl\")\n"
      ],
      "metadata": {
        "id": "c8vSC3kkf4tu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}