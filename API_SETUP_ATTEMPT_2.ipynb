{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3LW7m0TXm26"
      },
      "outputs": [],
      "source": [
        "from google.colab import files, pathlib\n",
        "up = files.upload()\n",
        "\n",
        "\n",
        "import os, shutil\n",
        "os.makedirs(\"models\", exist_ok=True)\n",
        "for fname in up.keys():\n",
        "    shutil.move(fname, f\"models/{fname}\")\n",
        "\n",
        "# check\n",
        "import glob\n",
        "print(\"Found models:\", glob.glob(\"models/*.pkl\"))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FastAPI\n",
        "import os, shutil, textwrap\n",
        "\n",
        "PROJECT = \"tweet_moderation_api\"\n",
        "if os.path.exists(PROJECT):\n",
        "    shutil.rmtree(PROJECT)\n",
        "os.makedirs(f\"{PROJECT}/app/static\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT}/models\", exist_ok=True)\n",
        "os.makedirs(f\"{PROJECT}/tests\", exist_ok=True)\n",
        "\n",
        "for f in [\"best_model_logreg_pipeline.pkl\", \"multi_model_pipeline.pkl\"]:\n",
        "    src = f\"models/{f}\"\n",
        "    assert os.path.exists(src), f\"{src} not found\"\n",
        "    shutil.copy(src, f\"{PROJECT}/models/{f}\")\n",
        "\n",
        "utils_code = \"\"\"\n",
        "import re\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "LEX_BAD = {\n",
        "    \"fuck\":1.5,\"fucking\":1.5,\"shit\":1.0,\"bitch\":1.3,\"slut\":1.5,\n",
        "    \"idiot\":0.8,\"stupid\":0.7,\"moron\":0.7,\"dumb\":0.6,\n",
        "    \"kill\":2.0,\"die\":1.5,\"murder\":2.2,\"hang\":2.0,\n",
        "    \"creep\":0.7,\"pervert\":1.0\n",
        "}\n",
        "\n",
        "IDENTITY_GROUPS = {\n",
        "    \"black\":\"Racist language\",\"blacks\":\"Racist language\",\n",
        "    \"asian\":\"Racist language\",\"asians\":\"Racist language\",\n",
        "    \"jew\":\"Antisemitic language\",\"jews\":\"Antisemitic language\",\n",
        "    \"muslim\":\"Islamophobic language\",\"muslims\":\"Islamophobic language\",\n",
        "    \"gay\":\"Homophobic language\",\"gays\":\"Homophobic language\",\n",
        "    \"lesbian\":\"Homophobic language\",\"lesbians\":\"Homophobic language\",\n",
        "}\n",
        "\n",
        "BASE_TAGS = {\n",
        "    \"toxic\":\"Abusive / toxic language\",\n",
        "    \"severe_toxic\":\"Severe abuse\",\n",
        "    \"obscene\":\"Obscene language\",\n",
        "    \"threat\":\"Violence / threats\",\n",
        "    \"insult\":\"Harassment / insult\",\n",
        "    \"identity_hate\":\"Hate speech / identity-based\"\n",
        "}\n",
        "\n",
        "def normalize(t:str)->str:\n",
        "    t=t.lower()\n",
        "    t=re.sub(r\"https?://\\\\S+|www\\\\.\\\\S+\",\" \",t)\n",
        "    t=re.sub(r\"[^a-z\\\\s]\",\" \",t)\n",
        "    t=re.sub(r\"\\\\s+\",\" \",t).strip()\n",
        "    return t\n",
        "\n",
        "def detect_identity_subtags(toks):\n",
        "    return sorted({IDENTITY_GROUPS[w] for w in toks if w in IDENTITY_GROUPS})\n",
        "\n",
        "def foulness_meter_0_10(text_norm:str,probs:np.ndarray,labels,thresholds):\n",
        "    base=6.0*float(np.max(probs))\n",
        "    toks=re.findall(r\"\\\\b[a-z]+\\\\b\",text_norm)\n",
        "    weights=[LEX_BAD[w] for w in toks if w in LEX_BAD]\n",
        "    lex=min(sum(weights),2.5)\n",
        "    cnt=Counter([w for w in toks if w in LEX_BAD])\n",
        "    rep_boost=min(0.5*(max(cnt.values())-1),1.0) if cnt else 0\n",
        "    num_on=sum(float(probs[i])>=thresholds[labels[i]] for i in range(len(labels)))\n",
        "    density=min(0.4*num_on,1.0)\n",
        "    return int(round(max(0,min(10,base+lex+rep_boost+density))))\n",
        "\"\"\"\n",
        "open(f\"{PROJECT}/app/utils.py\",\"w\").write(utils_code)\n",
        "\n",
        "\n",
        "main_code = \"\"\"\n",
        "from fastapi import FastAPI\n",
        "from fastapi.responses import FileResponse\n",
        "from fastapi.staticfiles import StaticFiles\n",
        "from pydantic import BaseModel\n",
        "import joblib,re,numpy as np\n",
        "from app.utils import normalize,foulness_meter_0_10,BASE_TAGS,detect_identity_subtags,IDENTITY_GROUPS\n",
        "\n",
        "BIN_ART=joblib.load(\"models/best_model_logreg_pipeline.pkl\")\n",
        "BIN_PIPE=BIN_ART[\"pipeline\"]\n",
        "BIN_THR=float(BIN_ART[\"threshold\"])\n",
        "\n",
        "ML_ART=joblib.load(\"models/multi_model_pipeline.pkl\")\n",
        "ML_PIPE=ML_ART[\"pipeline\"]\n",
        "LABELS=list(ML_ART[\"labels\"])\n",
        "THR={k:float(v) for k,v in ML_ART[\"thresholds\"].items()}\n",
        "\n",
        "app=FastAPI(title=\"Tweet Moderation API\",version=\"2.0\")\n",
        "app.mount(\"/static\",StaticFiles(directory=\"app/static\"),name=\"static\")\n",
        "\n",
        "class Input(BaseModel):\n",
        "    text:str\n",
        "\n",
        "@app.get(\"/\")\n",
        "def root(): return FileResponse(\"app/static/index.html\")\n",
        "\n",
        "@app.get(\"/health\")\n",
        "def health(): return {\"status\":\"ok\"}\n",
        "\n",
        "@app.post(\"/predict\")\n",
        "def predict(i:Input):\n",
        "    norm=normalize(i.text)\n",
        "    p_bin=float(BIN_PIPE.predict_proba([norm])[0,1])\n",
        "    bin_label=int(p_bin>=BIN_THR)\n",
        "    P=ML_PIPE.predict_proba([norm])[0]\n",
        "    subs={lab:{\"prob\":float(P[k]),\"label\":int(P[k]>=THR[lab])} for k,lab in enumerate(LABELS)}\n",
        "    tags=[BASE_TAGS[l] for l in LABELS if subs[l][\"label\"]]\n",
        "    toks=re.findall(r\"\\\\b[a-z]+\\\\b\",norm)\n",
        "    if subs.get(\"identity_hate\",{}).get(\"label\",0)==1 or any(t in IDENTITY_GROUPS for t in toks):\n",
        "        tags+=detect_identity_subtags(toks)\n",
        "        if \"Hate speech / identity-based\" not in tags: tags.append(\"Hate speech / identity-based\")\n",
        "    meter=foulness_meter_0_10(norm,np.array(P),LABELS,THR)\n",
        "    return {\"binary\":{\"prob\":p_bin,\"threshold\":BIN_THR,\"label\":bin_label},\"subtypes\":subs,\"tags\":tags,\"foulness_meter\":meter}\n",
        "\"\"\"\n",
        "open(f\"{PROJECT}/app/main.py\",\"w\").write(main_code)\n",
        "\n",
        "index_html = \"\"\"\n",
        "<!doctype html><html><head><meta charset='utf-8'/>\n",
        "<title>Tweet Moderation Demo</title>\n",
        "<style>body{font-family:sans-serif;margin:24px;max-width:800px}.pill{display:inline-block;background:#eee;padding:6px 10px;border-radius:999px;margin:3px}</style>\n",
        "</head><body><h1>Tweet Moderation Demo</h1>\n",
        "<textarea id='txt' rows=5 style='width:100%'></textarea><br>\n",
        "<button onclick='go()'>Analyze</button><span id='status'></span>\n",
        "<div><b>Foulness meter:</b> <span id='meter'></span>/10</div>\n",
        "<div id='tags'></div><pre id='json'></pre>\n",
        "<script>\n",
        "async function go(){\n",
        " let res=await fetch('/predict',{method:'POST',headers:{'Content-Type':'application/json'},body:JSON.stringify({text:txt.value})});\n",
        " let js=await res.json();\n",
        " meter.textContent=js.foulness_meter;\n",
        " tags.innerHTML=(js.tags||[]).map(t=>`<span class='pill'>${t}</span>`).join('');\n",
        " json.textContent=JSON.stringify(js,null,2);\n",
        "}\n",
        "</script></body></html>\n",
        "\"\"\"\n",
        "open(f\"{PROJECT}/app/static/index.html\",\"w\").write(index_html)\n",
        "\n",
        "open(f\"{PROJECT}/requirements.txt\",\"w\").write(\"\"\"fastapi==0.115.0\n",
        "uvicorn[standard]==0.30.6\n",
        "scikit-learn==1.4.2\n",
        "pandas==2.2.2\n",
        "numpy==1.26.4\n",
        "joblib==1.4.2\n",
        "pytest==8.3.3\n",
        "\"\"\")\n",
        "\n",
        "open(f\"{PROJECT}/tests/test_api.py\",\"w\").write(\"\"\"\n",
        "from fastapi.testclient import TestClient\n",
        "from app.main import app\n",
        "c=TestClient(app)\n",
        "\n",
        "def test_root():\n",
        "    assert c.get('/').status_code==200\n",
        "\n",
        "def test_predict():\n",
        "    r=c.post('/predict',json={'text':'you idiot'})\n",
        "    assert r.status_code==200\n",
        "    js=r.json()\n",
        "    assert 'foulness_meter' in js and 0 <= js['foulness_meter'] <= 10\n",
        "\"\"\")\n",
        "\n",
        "open(f\"{PROJECT}/Dockerfile\",\"w\").write(\"\"\"FROM python:3.11-slim\n",
        "WORKDIR /app\n",
        "COPY requirements.txt .\n",
        "RUN pip install --no-cache-dir -r requirements.txt\n",
        "COPY app app\n",
        "COPY models models\n",
        "EXPOSE 8000\n",
        "CMD [\"uvicorn\",\"app.main:app\",\"--host\",\"0.0.0.0\",\"--port\",\"8000\"]\n",
        "\"\"\")\n",
        "\n",
        "open(f\"{PROJECT}/README.md\",\"w\").write(\"\"\"\n",
        "# Tweet Moderation API\n",
        "\n",
        "FastAPI app to classify tweets as foul or proper, with multi-label subtypes, tags, and a 0â€“10 foulness meter.\n",
        "\n",
        "## Run locally\n",
        "pip install -r requirements.txt\n",
        "uvicorn app.main:app --reload --port 8000\n",
        "\n",
        "## Docker\n",
        "docker build -t tweet-moderation .\n",
        "docker run -p 8000:8000 tweet-moderation\n",
        "\"\"\")\n",
        "\n",
        "open(f\"{PROJECT}/app/__init__.py\",\"w\").write(\"\")\n",
        "\n",
        "print(\"Project generated successfully:\", PROJECT)\n"
      ],
      "metadata": {
        "id": "zgU_kCEKYWvz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT = \"tweet_moderation_api\"\n",
        "\n",
        "!pip install -q --no-cache-dir -r {PROJECT}/requirements.txt\n",
        "\n",
        "%cd {PROJECT}\n",
        "!pytest -q\n",
        "%cd ..\n"
      ],
      "metadata": {
        "id": "5pq0P65lZIFm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PROJECT = \"tweet_moderation_api\"\n",
        "\n",
        "!touch {PROJECT}/app/__init__.py\n",
        "\n",
        "conftest = \"\"\"\n",
        "import os, sys\n",
        "# Add project root (one level up from tests/) to sys.path so 'app' is importable\n",
        "ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), \"..\"))\n",
        "if ROOT not in sys.path:\n",
        "    sys.path.insert(0, ROOT)\n",
        "\"\"\"\n",
        "with open(f\"{PROJECT}/tests/conftest.py\",\"w\") as f:\n",
        "    f.write(conftest)\n",
        "\n",
        "%cd {PROJECT}\n",
        "!pytest -q\n",
        "%cd ..\n"
      ],
      "metadata": {
        "id": "ibSs5uqIZpZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, subprocess, sys\n",
        "from getpass import getpass\n",
        "\n",
        "PROJECT = \"tweet_moderation_api\"\n",
        "REPO_URL = \"https://github.com/BAKER-2/Tweet_Moderation_API.git\"\n",
        "\n",
        "token = getpass(\"Paste your GitHub Personal Access Token (hidden): \").strip()\n",
        "assert token, \"Empty token.\"\n",
        "\n",
        "os.chdir(PROJECT)\n",
        "subprocess.run([\"git\",\"init\"], check=True)\n",
        "subprocess.run([\"git\",\"config\",\"user.name\",\"BAKER-2\"], check=True)\n",
        "subprocess.run([\"git\",\"config\",\"user.email\",\"baker.husein01@gmail.com\"], check=True)\n",
        "subprocess.run([\"git\",\"branch\",\"-M\",\"main\"], check=True)\n",
        "subprocess.run([\"git\",\"add\",\"-A\"], check=True)\n",
        "subprocess.run([\"git\",\"commit\",\"-m\",\"API: FastAPI + models + tests + Docker\"], check=False)\n",
        "\n",
        "safe_remote = f\"https://{token}@github.com/BAKER-2/Tweet_Moderation_API.git\"\n",
        "subprocess.run([\"git\",\"remote\",\"remove\",\"origin\"], check=False)\n",
        "subprocess.run([\"git\",\"remote\",\"add\",\"origin\", safe_remote], check=True)\n",
        "\n",
        "# guthub push\n",
        "subprocess.run([\"git\",\"push\",\"-u\",\"origin\",\"main\",\"--force\"], check=True)\n",
        "\n",
        "print(\"\\n Pushed to GitHub: https://github.com/BAKER-2/Tweet_Moderation_API\")\n"
      ],
      "metadata": {
        "id": "_UphoEWkcq0i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}